{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we add all the Probe data to the dataset\n",
    "# https://atlas.ripe.net/docs/apis/rest-api-reference/#probes\n",
    "\n",
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "import platform\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Making the plots standard \n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-14.2.1-arm64-arm-64bit\n",
      "Python 3.10.11 (main, Apr  7 2023, 07:24:53) [Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "Pandas 2.0.1\n",
      "Scikit-Learn 1.4.0\n"
     ]
    }
   ],
   "source": [
    "#Hardware and software details \n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are your features ? \n",
    "### Traceroute Measurement Result\n",
    "- **af**: 4 or 6 (integer)\n",
    "- **dst_addr**: IP address of the destination (string)\n",
    "- **dst_name**: Name of the destination (string)\n",
    "- **endtime**: Unix timestamp for the end of measurement (int)\n",
    "- **from**: IP address of the probe as known by the controller (string)\n",
    "- **msm_id**: Measurement identifier (int)\n",
    "- **paris_id**: Variation for the Paris mode of traceroute (int)\n",
    "- **prb_id**: Source probe ID (int)\n",
    "- **proto**: \"UDP\" or \"ICMP\" (string)\n",
    "- **result**: List of hop elements (array)\n",
    "\n",
    "#### Hop Element\n",
    "- **hop**: Hop number (int)\n",
    "- **error**: [Optional] Error description when an error occurs trying to send a packet. No result structure in this case. (string)\n",
    "- **result**: Variable content, depending on the type of response (array)\n",
    "\n",
    "##### Case: Timeout\n",
    "- **x**: \"*\"\n",
    "\n",
    "##### Case: Reply\n",
    "- **err**: (Optional) Error ICMP: \"N\" (network unreachable), \"H\" (destination unreachable), \"A\" (administratively prohibited), \"P\" (protocol unreachable), \"p\" (port unreachable) (string)\n",
    "- **from**: IPv4 or IPv6 source address in reply (string)\n",
    "- **ittl**: (Optional) Time-to-live in the packet that triggered the error ICMP. Omitted if equal to 1 (int)\n",
    "- **late**: (Optional) Number of packets a reply is late; in this case, RTT is not present (int)\n",
    "- **mtu**: (Optional) Path MTU from a packet too big ICMP (int)\n",
    "- **rtt**: Round-trip-time of the reply, not present when the response is late (float)\n",
    "- **size**: Size of the reply (int)\n",
    "- **ttl**: Time-to-live in the reply (int)\n",
    "- **icmpext**: [Optional] Information when an ICMP header is found in the reply (object)\n",
    "  - **version**: RFC4884 version (int)\n",
    "  - **rfc4884**: \"1\" if length indication is present, \"0\" otherwise (int)\n",
    "  - **obj**: Elements of the object (array)\n",
    "  - **class**: RFC4884 class (int)\n",
    "  - **type**: RFC4884 type (int)\n",
    "\n",
    "- **size**: Packet size (int)\n",
    "- **src_addr**: Source address used by the probe (string)\n",
    "- **timestamp**: Unix timestamp for the start of measurement (int)\n",
    "- **type**: \"traceroute\" (string)\n",
    "- **fw**: Firmware version of the probe\n",
    "- **mver**: Version of measurement code. Format: \"x.y.z\" (string)\n",
    "- **lts**: Last time synchronized. How long ago (in seconds) was the probeâ€™s clock in sync with that of a controller. Value -1 indicates the probe doesn't know if it's in sync (int)\n",
    "- **msm_name**: Measurement type \"Ping\" (string)\n",
    "- **stored_timestamp**: Time when the measurement results were stored or recorded by RIPE Atlas servers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pickled dataframes you will be using for the experiments\n",
    "# the experiment_df has the traceroute data with no probe data  - it has the last_rtt,dst_id and distance\n",
    "# the probe_df has the probe data\n",
    "latency_df = pd.read_pickle('latency_noprobe.pickle')\n",
    "probe_df = pd.read_pickle('probe_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the null values from the experiment_df \n",
    "nan_rows = latency_df[latency_df['last_rtt'].isnull()]\n",
    "latency_df = latency_df.dropna(subset=['last_rtt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fw', 'mver', 'lts', 'endtime', 'dst_name', 'dst_addr', 'src_addr',\n",
       "       'proto', 'af', 'size', 'paris_id', 'result', 'msm_id', 'prb_id',\n",
       "       'timestamp', 'msm_name', 'from', 'type', 'group_id', 'stored_timestamp',\n",
       "       'destination_ip_responded', 'new_time', 'dst_id', 'dst_names',\n",
       "       'src_names', 'Hour', 'distance', 'hop', 'hop_ip', 'rtt', 'unique_ips',\n",
       "       'avg_rtt', 'source_longitude', 'source_latitude',\n",
       "       'destination_longitude', 'destination_latitude', 'date', 'last_rtt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminate rows where last_rtt is greater than 100ms \n",
    "latency_df = latency_df[latency_df['last_rtt'] < 100]\n",
    "latency_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the number of hops to the dataframe\n",
    "#collect the last hop from each index and add it to the dataframe\n",
    "latency_df['hop_count'] = latency_df.groupby(latency_df.index)['hop'].transform('last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hop</th>\n",
       "      <th>hop_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hop  hop_count\n",
       "2    1          8\n",
       "2    2          8\n",
       "2    3          8\n",
       "2    4          8\n",
       "2    5          8\n",
       "2    6          8\n",
       "2    7          8\n",
       "2    8          8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = latency_df[latency_df.index == 2]\n",
    "q[['hop','hop_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the latency columns that are not required for the experiments\n",
    "#these were added to the dataframe to calculate the last_rtt  - the rtt for the traceoute is in the last_rtt column\n",
    "latency_df = latency_df.drop(['result','Hour','hop','hop_ip','rtt','avg_rtt','unique_ips','dst_names','type','dst_name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fw', 'mver', 'lts', 'endtime', 'dst_addr', 'src_addr', 'proto', 'af',\n",
       "       'size', 'paris_id', 'msm_id', 'prb_id', 'timestamp', 'msm_name', 'from',\n",
       "       'group_id', 'stored_timestamp', 'destination_ip_responded', 'new_time',\n",
       "       'dst_id', 'src_names', 'distance', 'source_longitude',\n",
       "       'source_latitude', 'destination_longitude', 'destination_latitude',\n",
       "       'date', 'last_rtt', 'hop_count', 'ProbeID', 'ASN', 'CountryCode',\n",
       "       'IPAddress', 'source_status', 'Anchor', 'Latitude', 'Longitude', 'Tags',\n",
       "       'Public', 'Since', 'Uptime', 'FirstConnected', 'Prefix_v4',\n",
       "       'LastConnected', 'Uptime(days)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add the source probe information to the experiment data \n",
    "source_merged_df = pd.merge(latency_df, probe_df, left_on='prb_id', right_on='ProbeID', how='left')\n",
    "source_merged_df = source_merged_df.rename(columns={'Status': 'source_status'})\n",
    "source_merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fw', 'mver', 'lts', 'endtime', 'dst_addr', 'src_addr', 'proto', 'af',\n",
       "       'size', 'paris_id', 'msm_id', 'prb_id', 'timestamp', 'msm_name', 'from',\n",
       "       'group_id', 'stored_timestamp', 'destination_ip_responded', 'new_time',\n",
       "       'dst_id', 'src_names', 'distance', 'source_longitude',\n",
       "       'source_latitude', 'destination_longitude', 'destination_latitude',\n",
       "       'date', 'last_rtt', 'hop_count', 'ProbeID_source', 'ASN_source',\n",
       "       'CountryCode_source', 'IPAddress_source', 'source_status',\n",
       "       'Anchor_source', 'Latitude_source', 'Longitude_source', 'Tags_source',\n",
       "       'Public_source', 'Since_source', 'Uptime_source',\n",
       "       'FirstConnected_source', 'Prefix_v4_source', 'LastConnected_source',\n",
       "       'Uptime(days)_source', 'ProbeID_destination', 'ASN_destination',\n",
       "       'CountryCode_destination', 'IPAddress_destination',\n",
       "       'destination_status', 'Anchor_destination', 'Latitude_destination',\n",
       "       'Longitude_destination', 'Tags_destination', 'Public_destination',\n",
       "       'Since_destination', 'Uptime_destination', 'FirstConnected_destination',\n",
       "       'Prefix_v4_destination', 'LastConnected_destination',\n",
       "       'Uptime(days)_destination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the destination probe data to the experiment data \n",
    "source_merged_df['dst_id'] = source_merged_df['dst_id'].astype('int64')\n",
    "destination_merged_df = pd.merge(source_merged_df, probe_df, left_on='dst_id', right_on='ProbeID', how='left', suffixes=('_source', '_destination'))\n",
    "destination_merged_df = destination_merged_df.rename(columns={'Status': 'destination_status'})\n",
    "destination_merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mver', 'dst_addr', 'src_addr', 'proto', 'msm_name', 'from',\n",
       "       'destination_ip_responded', 'src_names', 'date', 'CountryCode_source',\n",
       "       'IPAddress_source', 'source_status', 'Tags_source', 'Prefix_v4_source',\n",
       "       'CountryCode_destination', 'IPAddress_destination',\n",
       "       'destination_status', 'Tags_destination', 'Prefix_v4_destination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what are the data types of the columns in the dataframe\n",
    "#check the object columns - these will need modification for the experiments\n",
    "object_columns = destination_merged_df.select_dtypes(include=['object']).columns\n",
    "df_subset = destination_merged_df[object_columns]\n",
    "df_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fw', 'lts', 'endtime', 'af', 'size', 'paris_id', 'msm_id', 'prb_id',\n",
       "       'timestamp', 'group_id', 'stored_timestamp', 'dst_id', 'hop_count',\n",
       "       'ProbeID_source', 'ASN_source', 'Uptime_source', 'ProbeID_destination',\n",
       "       'ASN_destination', 'Uptime_destination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the int columns - these dont need modification \n",
    "int_columns = destination_merged_df.select_dtypes(include=['int64']).columns\n",
    "df_subset2 = destination_merged_df[int_columns]\n",
    "df_subset2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['distance', 'source_longitude', 'source_latitude',\n",
       "       'destination_longitude', 'destination_latitude', 'last_rtt',\n",
       "       'Latitude_source', 'Longitude_source', 'Uptime(days)_source',\n",
       "       'Latitude_destination', 'Longitude_destination',\n",
       "       'Uptime(days)_destination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find float data types\n",
    "float_columns = destination_merged_df.select_dtypes(include=['float']).columns\n",
    "df_subset3 = destination_merged_df[float_columns]\n",
    "df_subset3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Anchor_source', 'Public_source', 'Anchor_destination',\n",
       "       'Public_destination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find boolean data types\n",
    "bool_columns = destination_merged_df.select_dtypes(include=['bool']).columns\n",
    "df_subset4 = destination_merged_df[bool_columns]\n",
    "df_subset4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['new_time', 'Since_source', 'FirstConnected_source',\n",
       "       'LastConnected_source', 'Since_destination',\n",
       "       'FirstConnected_destination', 'LastConnected_destination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find datetime data types\n",
    "datetime_columns = destination_merged_df.select_dtypes(include=['datetime']).columns\n",
    "df_subset5 = destination_merged_df[datetime_columns]\n",
    "df_subset5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add day of the week as an integer to the dataframe\n",
    "\n",
    "destination_merged_df['day_of_week'] = destination_merged_df['new_time'].dt.dayofweek\n",
    "\n",
    "#add hour of the day as an integer to the dataframe\n",
    "destination_merged_df['hour_of_day'] = destination_merged_df['new_time'].dt.hour\n",
    "\n",
    "#add minute of the hour as an integer to the dataframe\n",
    "destination_merged_df['minute_of_hour'] = destination_merged_df['new_time'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling the date columns\n",
    "#destination_merged_df[df_subset5.columns]\n",
    "\n",
    "#drop the first connected, lastconnected columns since we may not need them \n",
    "cols_to_drop = ['FirstConnected_source','LastConnected_source','FirstConnected_destination','LastConnected_destination']\n",
    "destination_merged_df = destination_merged_df.drop(columns=cols_to_drop,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the last value in the 'new_time' column\n",
    "last_new_time = destination_merged_df['new_time'].iloc[-1]\n",
    "\n",
    "#subtract the last value from the 'new_time' column to get the time since the last connection status change\n",
    "destination_merged_df['source_status_change(days)'] = last_new_time - destination_merged_df['Since_source']\n",
    "\n",
    "#convert this value to days\n",
    "destination_merged_df['source_status_change(days)'] = destination_merged_df['source_status_change(days)'].dt.total_seconds()/ (24 * 60 * 60)\n",
    "\n",
    "#drop the 'Since_source' column\n",
    "destination_merged_df = destination_merged_df.drop(columns=['Since_source'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract the last value from the 'new_time' column to get the time since the last connection status change\n",
    "destination_merged_df['destination_status_change(days)'] = last_new_time - destination_merged_df['Since_destination']\n",
    "\n",
    "#convert this value to days\n",
    "destination_merged_df['destination_status_change(days)'] = destination_merged_df['destination_status_change(days)'].dt.total_seconds()/ (24 * 60 * 60)\n",
    "\n",
    "#drop the 'Since_destination' column\n",
    "destination_merged_df = destination_merged_df.drop(columns=['Since_destination'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the 'new_time' column - the same data is in the 'timestamp' column\n",
    "destination_merged_df = destination_merged_df.drop(columns=['new_time'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert the timestamp column to datetime\n",
    "destination_merged_df['timestamp'] = pd.to_datetime(destination_merged_df['timestamp'])\n",
    "\n",
    "# Normalise the timestamp column \n",
    "initial_timestamp = destination_merged_df['timestamp'].min()\n",
    "\n",
    "# Create a new column 'normalized_timestamp' with the normalized values\n",
    "destination_merged_df['norm_timestamp'] = destination_merged_df['timestamp'] - initial_timestamp\n",
    "\n",
    "#convert the normalized timestamp to seconds\n",
    "destination_merged_df['norm_timestamp'] = destination_merged_df['norm_timestamp'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert the timestamp column to datetime\n",
    "destination_merged_df['stored_timestamp'] = pd.to_datetime(destination_merged_df['stored_timestamp'])\n",
    "\n",
    "# Normalise the timestamp column \n",
    "initial_timestamp = destination_merged_df['stored_timestamp'].min()\n",
    "\n",
    "# Create a new column 'normalized_timestamp' with the normalized values\n",
    "destination_merged_df['norm_storedtimestamp'] = destination_merged_df['stored_timestamp'] - initial_timestamp\n",
    "\n",
    "#convert the timestamp to seconds\n",
    "destination_merged_df['norm_storedtimestamp'] = destination_merged_df['norm_storedtimestamp'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the 'timestamp' and 'stored_timestamp' columns\n",
    "destination_merged_df = destination_merged_df.drop(columns=['timestamp','stored_timestamp'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with the IP columns \n",
    "# src_addr,dst_addr,from,IPAddress_source,IPAddress_destination,Prefix_v4_source,Prefix_v4_destination\n",
    "# we drop all the IP columns and keep the prefix columns\n",
    "destination_merged_df = destination_merged_df.drop(columns=['src_addr','dst_addr','from','IPAddress_source','IPAddress_destination'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split IP address into octets and mask\n",
    "def split_prefix(prefix):\n",
    "    ip, mask = prefix.split('/')\n",
    "    octets = ip.split('.')\n",
    "    return octets + [mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'Prefix' column\n",
    "destination_merged_df[['src_Octet1', 'src_Octet2', 'src_Octet3', 'src_Octet4', 'src_Mask']] = destination_merged_df['Prefix_v4_source'].apply(split_prefix).apply(pd.Series)\n",
    "# Convert the new columns to integer type\n",
    "destination_merged_df[['src_Octet1', 'src_Octet2', 'src_Octet3', 'src_Octet4', 'src_Mask']] = destination_merged_df[['src_Octet1', 'src_Octet2', 'src_Octet3', 'src_Octet4', 'src_Mask']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_merged_df[['dst_Octet1', 'dst_Octet2', 'dst_Octet3', 'dst_Octet4', 'dst_Mask']] = destination_merged_df['Prefix_v4_destination'].apply(split_prefix).apply(pd.Series)\n",
    "destination_merged_df[['dst_Octet1', 'dst_Octet2', 'dst_Octet3', 'dst_Octet4', 'dst_Mask']]  = destination_merged_df[['dst_Octet1', 'dst_Octet2', 'dst_Octet3', 'dst_Octet4', 'dst_Mask']] .astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_merged_df = destination_merged_df.drop(columns=['Prefix_v4_source','Prefix_v4_destination'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling the Object datatype \n",
    "#drop columns added from the probe dataframe that are not required for the experiments\n",
    "cols_to_drop = ['Tags_source','Tags_destination']\n",
    "destination_merged_df = destination_merged_df.drop(columns=cols_to_drop,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1238890, 29) (1238890, 60)\n"
     ]
    }
   ],
   "source": [
    "#adding labels to categorical features of interest \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#create a copy of the latency_df\n",
    "model_df = destination_merged_df.copy()\n",
    "\n",
    "categorical_cols = ['mver', 'proto', 'msm_name', 'destination_ip_responded','CountryCode_source','CountryCode_destination','source_status','destination_status']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    model_df[col] = le.fit_transform(model_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "#check the shape of the final dataframe\n",
    "print(latency_df.shape, model_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping for mver:\n",
      "{'2.6.2': 0, nan: 1}\n",
      "Label Mapping for proto:\n",
      "{'ICMP': 0}\n",
      "Label Mapping for msm_name:\n",
      "{'Traceroute': 0}\n",
      "Label Mapping for destination_ip_responded:\n",
      "{False: 0, True: 1, nan: 2}\n",
      "Label Mapping for CountryCode_source:\n",
      "{'ES': 0}\n",
      "Label Mapping for CountryCode_destination:\n",
      "{'ES': 0}\n",
      "Label Mapping for source_status:\n",
      "{'Connected': 0, 'Disconnected': 1}\n",
      "Label Mapping for destination_status:\n",
      "{'Connected': 0}\n"
     ]
    }
   ],
   "source": [
    "# Access the mapping of labels to numerical values\n",
    "for col, encoder in label_encoders.items():\n",
    "    label_mapping = {label: index for index, label in enumerate(encoder.classes_)}\n",
    "    print(f\"Label Mapping for {col}:\\n{label_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fw', 'mver', 'lts', 'endtime', 'proto', 'af', 'size', 'paris_id',\n",
       "       'msm_id', 'prb_id', 'msm_name', 'group_id', 'destination_ip_responded',\n",
       "       'dst_id', 'src_names', 'distance', 'source_longitude',\n",
       "       'source_latitude', 'destination_longitude', 'destination_latitude',\n",
       "       'date', 'last_rtt', 'hop_count', 'ProbeID_source', 'ASN_source',\n",
       "       'CountryCode_source', 'source_status', 'Anchor_source',\n",
       "       'Latitude_source', 'Longitude_source', 'Public_source', 'Uptime_source',\n",
       "       'Uptime(days)_source', 'ProbeID_destination', 'ASN_destination',\n",
       "       'CountryCode_destination', 'destination_status', 'Anchor_destination',\n",
       "       'Latitude_destination', 'Longitude_destination', 'Public_destination',\n",
       "       'Uptime_destination', 'Uptime(days)_destination', 'day_of_week',\n",
       "       'hour_of_day', 'minute_of_hour', 'source_status_change(days)',\n",
       "       'destination_status_change(days)', 'norm_timestamp',\n",
       "       'norm_storedtimestamp', 'src_Octet1', 'src_Octet2', 'src_Octet3',\n",
       "       'src_Octet4', 'src_Mask', 'dst_Octet1', 'dst_Octet2', 'dst_Octet3',\n",
       "       'dst_Octet4', 'dst_Mask'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns that are not required for the experiments\n",
    "cols_to_drop = ['mver','endtime','msm_id','group_id','ProbeID_source','ProbeID_destination','Uptime_destination','Uptime_source']\n",
    "model_df = model_df.drop(columns=cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe to a pickle file\n",
    "model_df.to_pickle('latency_withprobe.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
